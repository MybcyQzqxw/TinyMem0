#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®°å¿†ç³»ç»Ÿè¯„æµ‹è„šæœ¬
ç”¨äºåœ¨ LoCoMo æ•°æ®é›†ä¸Šè¯„æµ‹å„ç§è®°å¿†ç³»ç»Ÿçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬QAå’ŒEvidenceæ£€ç´¢æŒ‡æ ‡

æ”¯æŒçš„æ¨¡å‹ç³»ç»Ÿï¼š
- tinymem0: TinyMem0 è®°å¿†ç³»ç»Ÿ
- (æœªæ¥å¯æ‰©å±•å…¶ä»–æ¨¡å‹)

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/evaluate_system.py --model tinymem0 --num-samples 5
"""

import json
import os
import sys
import subprocess
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from tqdm import tqdm
import argparse
from importlib import import_module

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œsrcç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

from locomo.task_eval.evaluation import eval_question_answering, f1_score, exact_match_score
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ¨¡å‹ç³»ç»Ÿæ˜ å°„è¡¨
AVAILABLE_MODELS = {
    'tinymem0': {
        'name': 'TinyMem0',
        'description': 'TinyMem0 è®°å¿†ç³»ç»Ÿ (æ”¯æŒæœ¬åœ°LLMå’Œé˜¿é‡Œäº‘API)',
        'module': 'tinymem0',
        'class': 'MemorySystem'
    },
    # æœªæ¥å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹ï¼Œä¾‹å¦‚ï¼š
    # 'mem0ai': {
    #     'name': 'Mem0.ai',
    #     'description': 'Mem0.ai å®˜æ–¹å®ç°',
    #     'module': 'mem0ai',
    #     'class': 'MemorySystem'
    # },
}

class MemorySystemEvaluator:
    """
    è®°å¿†ç³»ç»Ÿè¯„æµ‹å™¨
    è´Ÿè´£å°†å¯¹è¯æ•°æ®è¾“å…¥è®°å¿†ç³»ç»Ÿï¼Œå¹¶è¯„ä¼°QAå’ŒEvidenceæ£€ç´¢æ€§èƒ½
    """
    
    def __init__(self, memory_system: Any, user_id: str = "eval_user", agent_id: str = "eval_agent"):
        """
        Args:
            memory_system: ä»»æ„è®°å¿†ç³»ç»Ÿå®ä¾‹ï¼ˆæ”¯æŒ write_memory å’Œ search_memory æ¥å£ï¼‰
            user_id: è¯„æµ‹ç”¨æˆ·ID
            agent_id: è¯„æµ‹ä»£ç†ID
        """
        self.memory_system = memory_system
        self.user_id = user_id
        self.agent_id = agent_id
        
    def load_conversation_into_memory(self, conversation_data: Dict[str, Any]) -> None:
        """
        å°†LoCoMoå¯¹è¯æ•°æ®åŠ è½½åˆ°TinyMem0è®°å¿†ç³»ç»Ÿä¸­
        
        Args:
            conversation_data: LoCoMoæ ¼å¼çš„å¯¹è¯æ•°æ®
        """
        print(f"Loading conversation {conversation_data.get('sample_id', 'unknown')} into memory...")
        
        # è·å–æ‰€æœ‰sessionï¼ŒæŒ‰æ—¶é—´é¡ºåºå¤„ç†
        sessions = []
        for key in conversation_data['conversation'].keys():
            if key.startswith('session_') and not key.endswith('_date_time'):
                session_num = int(key.split('_')[1])
                date_key = f"session_{session_num}_date_time"
                sessions.append((session_num, conversation_data['conversation'].get(date_key, ""), conversation_data['conversation'][key]))
        
        # æŒ‰sessionç¼–å·æ’åº
        sessions.sort(key=lambda x: x[0])
        
        # é€ä¸ªå¤„ç†æ¯ä¸ªsessionä¸­çš„å¯¹è¯
        for session_num, date_time, dialogs in sessions:
            print(f"Processing session {session_num} ({date_time})...")
            
            for dialog_idx, dialog in enumerate(dialogs):
                speaker = dialog['speaker']
                text = dialog['text']
                
                # æ„å»ºå®Œæ•´çš„å¯¹è¯æ–‡æœ¬
                conversation_text = f"{speaker}: {text}"
                
                # å¦‚æœæœ‰å›¾ç‰‡æè¿°ï¼ŒåŠ å…¥åˆ°å¯¹è¯ä¸­
                if 'blip_caption' in dialog and dialog['blip_caption']:
                    conversation_text += f" [åˆ†äº«äº†å›¾ç‰‡: {dialog['blip_caption']}]"
                
                # æ„å»ºmetadataï¼ŒåŒ…å«sessionå’Œdialogä¿¡æ¯ç”¨äºevidenceè¿½è¸ª
                extra_metadata = {
                    'session_id': session_num,
                    'dialog_id': dialog_idx + 1,  # dialogä»1å¼€å§‹ç¼–å·
                    'date_time': date_time
                }
                
                # å°†å¯¹è¯è¾“å…¥åˆ°è®°å¿†ç³»ç»Ÿ
                try:
                    self.memory_system.write_memory(
                        conversation=conversation_text,
                        user_id=self.user_id,
                        agent_id=self.agent_id,
                        extra_metadata=extra_metadata
                    )
                except Exception as e:
                    print(f"Error processing dialog {dialog.get('dia_id', 'unknown')}: {e}")
                    continue
    
    def answer_question(self, question: str, context_limit: int = 10) -> Tuple[str, List[str]]:
        """
        åŸºäºè®°å¿†ç³»ç»Ÿå›ç­”é—®é¢˜
        
        Args:
            question: è¦å›ç­”çš„é—®é¢˜
            context_limit: æ£€ç´¢çš„è®°å¿†æ•°é‡é™åˆ¶
            
        Returns:
            (å›ç­”æ–‡æœ¬, æ£€ç´¢åˆ°çš„evidenceåˆ—è¡¨)
        """
        try:
            # ä»è®°å¿†ç³»ç»Ÿä¸­æ£€ç´¢ç›¸å…³è®°å¿†
            memories = self.memory_system.search_memory(
                query=question,
                user_id=self.user_id,
                agent_id=self.agent_id,
                limit=context_limit
            )
            
            if not memories:
                return "No information available", []
            
            # æå–evidenceä¿¡æ¯ï¼ˆä»memoryçš„metadataä¸­ï¼‰
            retrieved_evidence = []
            for memory in memories:
                # å°è¯•ä»metadataä¸­æå–sessionå’Œdialogä¿¡æ¯
                if 'metadata' in memory:
                    meta = memory['metadata']
                    if 'session_id' in meta and 'dialog_id' in meta:
                        evidence_id = f"D{meta['session_id']}:{meta['dialog_id']}"
                        retrieved_evidence.append(evidence_id)
            
            # æ„å»ºcontext
            context_parts = []
            for memory in memories:
                context_parts.append(f"Memory: {memory['text']} (similarity: {memory['score']:.3f})")
            
            context = "\n".join(context_parts)
            
            # ä½¿ç”¨LLMå›ç­”é—®é¢˜
            from tinymem0.adapters import call_llm_with_prompt
            from tinymem0.prompts import QA_SYSTEM_PROMPT, build_qa_prompt
            
            qa_prompt = build_qa_prompt(context, question)
            
            answer = call_llm_with_prompt(
                self.memory_system.llm_model,
                QA_SYSTEM_PROMPT,
                qa_prompt
            )
            
            return (answer.strip() if answer else "No information available", retrieved_evidence)
            
        except Exception as e:
            print(f"Error answering question '{question}': {e}")
            return "No information available", []
    
    def evaluate_qa_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªLoCoMoæ ·æœ¬çš„QAä»»åŠ¡
        
        Args:
            sample: LoCoMoæ ·æœ¬æ•°æ®
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœå’Œè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        # å…ˆåŠ è½½å¯¹è¯åˆ°è®°å¿†ç³»ç»Ÿ
        self.load_conversation_into_memory(sample)
        
        # å›ç­”æ‰€æœ‰é—®é¢˜
        results = []
        for qa_item in sample['qa']:
            question = qa_item['question']
            ground_truth = str(qa_item['answer'])
            category = qa_item.get('category', 1)
            gold_evidence = qa_item.get('evidence', [])
            
            # è·å–æ¨¡å‹å›ç­”å’Œæ£€ç´¢çš„evidence
            prediction, retrieved_evidence = self.answer_question(question)
            
            # è®¡ç®—QAè¯„ä¼°æŒ‡æ ‡
            if category in [2, 3, 4]:  # single-hop, temporal, open-domain
                f1 = f1_score(prediction, ground_truth)
            elif category == 1:  # multi-hop
                f1 = self._multi_answer_f1(prediction, ground_truth)
            elif category == 5:  # adversarial
                f1 = 1.0 if 'no information available' in prediction.lower() or 'not mentioned' in prediction.lower() else 0.0
            else:
                f1 = f1_score(prediction, ground_truth)
            
            em = exact_match_score(prediction, ground_truth)
            
            # è®¡ç®—Evidenceè¯„ä¼°æŒ‡æ ‡
            evidence_metrics = self._calculate_evidence_metrics(retrieved_evidence, gold_evidence)
            recall_at_5 = self._calculate_recall_at_k(retrieved_evidence, gold_evidence, k=5)
            recall_at_10 = self._calculate_recall_at_k(retrieved_evidence, gold_evidence, k=10)
            mrr = self._calculate_mrr(retrieved_evidence, gold_evidence)
            
            result_item = {
                'question': question,
                'answer': ground_truth,
                'prediction': prediction,
                'category': category,
                'f1_score': f1,
                'exact_match': em,
                'evidence': gold_evidence,
                'retrieved_evidence': retrieved_evidence,
                # Evidenceè¯„ä¼°æŒ‡æ ‡
                'evidence_precision': evidence_metrics['precision'],
                'evidence_recall': evidence_metrics['recall'],
                'evidence_f1': evidence_metrics['f1'],
                'recall_at_5': recall_at_5,
                'recall_at_10': recall_at_10,
                'mrr': mrr
            }
            results.append(result_item)
        
        return {
            'sample_id': sample.get('sample_id', 'unknown'),
            'qa_results': results
        }
    
    def _multi_answer_f1(self, prediction: str, ground_truth: str) -> float:
        """
        è®¡ç®—å¤šç­”æ¡ˆçš„F1åˆ†æ•°ï¼ˆç”¨äºcategory 1çš„é—®é¢˜ï¼‰
        """
        from locomo.task_eval.evaluation import f1
        return float(f1(prediction, ground_truth))
    
    def _calculate_evidence_metrics(self, retrieved_evidence: List[str], gold_evidence: List[str]) -> Dict[str, float]:
        """
        è®¡ç®—Evidence Precision, Recall, F1
        
        Args:
            retrieved_evidence: ç³»ç»Ÿæ£€ç´¢å‡ºçš„evidenceåˆ—è¡¨
            gold_evidence: æ ‡å‡†ç­”æ¡ˆçš„evidenceåˆ—è¡¨
            
        Returns:
            åŒ…å«precision, recall, f1çš„å­—å…¸
        """
        if not retrieved_evidence or not gold_evidence:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        
        # è½¬æ¢ä¸ºé›†åˆä»¥ä¾¿è®¡ç®—äº¤é›†
        retrieved_set = set(retrieved_evidence)
        gold_set = set(gold_evidence)
        
        # è®¡ç®—äº¤é›†
        common = retrieved_set & gold_set
        
        if len(common) == 0:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        
        # è®¡ç®—precisionå’Œrecall
        precision = len(common) / len(retrieved_set)
        recall = len(common) / len(gold_set)
        
        # è®¡ç®—F1
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    
    def _calculate_recall_at_k(self, retrieved_evidence: List[str], gold_evidence: List[str], k: int = 5) -> float:
        """
        è®¡ç®—Recall@Kï¼šå‰Kæ¡æ£€ç´¢ç»“æœä¸­æ˜¯å¦åŒ…å«gold evidence
        
        Args:
            retrieved_evidence: ç³»ç»Ÿæ£€ç´¢å‡ºçš„evidenceåˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
            gold_evidence: æ ‡å‡†ç­”æ¡ˆçš„evidenceåˆ—è¡¨
            k: å–å‰Kæ¡
            
        Returns:
            Recall@Kåˆ†æ•°ï¼ˆ0æˆ–1ï¼‰
        """
        if not gold_evidence:
            return 1.0  # å¦‚æœæ²¡æœ‰gold evidenceï¼Œé»˜è®¤ä¸º1
        
        if not retrieved_evidence:
            return 0.0
        
        # å–å‰Kæ¡
        top_k = retrieved_evidence[:k]
        top_k_set = set(top_k)
        gold_set = set(gold_evidence)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰äº¤é›†
        return 1.0 if len(top_k_set & gold_set) > 0 else 0.0
    
    def _calculate_mrr(self, retrieved_evidence: List[str], gold_evidence: List[str]) -> float:
        """
        è®¡ç®—MRRï¼ˆMean Reciprocal Rankï¼‰ï¼šgold evidenceåœ¨æ£€ç´¢ç»“æœä¸­çš„æ’åå€’æ•°
        
        Args:
            retrieved_evidence: ç³»ç»Ÿæ£€ç´¢å‡ºçš„evidenceåˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
            gold_evidence: æ ‡å‡†ç­”æ¡ˆçš„evidenceåˆ—è¡¨
            
        Returns:
            MRRåˆ†æ•°
        """
        if not gold_evidence or not retrieved_evidence:
            return 0.0
        
        gold_set = set(gold_evidence)
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå‘½ä¸­çš„ä½ç½®
        for rank, evidence in enumerate(retrieved_evidence, start=1):
            if evidence in gold_set:
                return 1.0 / rank
        
        return 0.0  # æ²¡æœ‰å‘½ä¸­
    


class LoCoMoEvaluator:
    """
    LoCoMoåŸºå‡†æµ‹è¯•è¯„ä¼°å™¨
    """
    
    def __init__(self, data_file: str):
        self.data_file = data_file
        self.samples = self._load_data()
    
    def _load_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½LoCoMoæ•°æ®"""
        with open(self.data_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def evaluate_memory_system(self, model_name: str, output_file: str, sample_ids: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        è¯„ä¼°æŒ‡å®šçš„è®°å¿†ç³»ç»Ÿæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§° (ä¾‹å¦‚: 'tinymem0')
            output_file: ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„
            sample_ids: è¦è¯„ä¼°çš„æ ·æœ¬IDåˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºè¯„ä¼°æ‰€æœ‰æ ·æœ¬ï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœç»Ÿè®¡
        """
        # åŠ¨æ€åŠ è½½æ¨¡å‹ç³»ç»Ÿ
        if model_name not in AVAILABLE_MODELS:
            raise ValueError(f"Unknown model: {model_name}. Available models: {list(AVAILABLE_MODELS.keys())}")
        
        model_config = AVAILABLE_MODELS[model_name]
        print(f"\n{'='*60}")
        print(f"è¯„ä¼°æ¨¡å‹: {model_config['name']}")
        print(f"æè¿°: {model_config['description']}")
        print(f"{'='*60}\n")
        
        # åŠ¨æ€å¯¼å…¥æ¨¡å‹ç±»
        try:
            module = import_module(model_config['module'])
            MemorySystemClass = getattr(module, model_config['class'])
        except (ImportError, AttributeError) as e:
            print(f"Error: Failed to load model {model_name}: {e}")
            print(f"Please ensure the module '{model_config['module']}' is properly installed in src/")
            return {}
        
        # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
        memory_system = MemorySystemClass(
            log_level="info",
            log_mode="plain"
        )
        
        # åˆ›å»ºè¯„æµ‹å™¨
        evaluator = MemorySystemEvaluator(memory_system)
        
        # ç­›é€‰è¦è¯„ä¼°çš„æ ·æœ¬
        eval_samples = self.samples
        if sample_ids:
            eval_samples = [s for s in self.samples if s.get('sample_id') in sample_ids]
        
        print(f"Evaluating {len(eval_samples)} samples...")
        
        all_results = []
        all_f1_scores = []
        all_em_scores = []
        all_evidence_precision = []
        all_evidence_recall = []
        all_evidence_f1 = []
        all_recall_at_5 = []
        all_recall_at_10 = []
        all_mrr = []
        category_stats = {}
        
        for sample in tqdm(eval_samples, desc="Evaluating samples"):
            try:
                # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºæ–°çš„è®°å¿†ç³»ç»Ÿå®ä¾‹ï¼ˆé¿å…æ ·æœ¬é—´å¹²æ‰°ï¼‰
                sample_id = sample.get('sample_id', 'unknown')
                sample_memory_system = MemorySystemClass(
                    collection_name=f"memories_{sample_id}",
                    log_level="warn",  # å‡å°‘æ—¥å¿—è¾“å‡º
                    qdrant_path=f"./qdrant_data_{sample_id}"  # ä¸ºæ¯ä¸ªæ ·æœ¬ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®ç›®å½•
                )
                sample_evaluator = MemorySystemEvaluator(sample_memory_system)
                
                # è¯„ä¼°æ ·æœ¬
                result = sample_evaluator.evaluate_qa_sample(sample)
                all_results.append(result)
                
                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                for qa_result in result['qa_results']:
                    all_f1_scores.append(qa_result['f1_score'])
                    all_em_scores.append(qa_result['exact_match'])
                    all_evidence_precision.append(qa_result['evidence_precision'])
                    all_evidence_recall.append(qa_result['evidence_recall'])
                    all_evidence_f1.append(qa_result['evidence_f1'])
                    all_recall_at_5.append(qa_result['recall_at_5'])
                    all_recall_at_10.append(qa_result['recall_at_10'])
                    all_mrr.append(qa_result['mrr'])
                    
                    category = qa_result['category']
                    if category not in category_stats:
                        category_stats[category] = {
                            'f1_scores': [], 'em_scores': [], 
                            'evidence_precision': [], 'evidence_recall': [], 'evidence_f1': [],
                            'recall_at_5': [], 'recall_at_10': [], 'mrr': [],
                            'count': 0
                        }
                    
                    category_stats[category]['f1_scores'].append(qa_result['f1_score'])
                    category_stats[category]['em_scores'].append(qa_result['exact_match'])
                    category_stats[category]['evidence_precision'].append(qa_result['evidence_precision'])
                    category_stats[category]['evidence_recall'].append(qa_result['evidence_recall'])
                    category_stats[category]['evidence_f1'].append(qa_result['evidence_f1'])
                    category_stats[category]['recall_at_5'].append(qa_result['recall_at_5'])
                    category_stats[category]['recall_at_10'].append(qa_result['recall_at_10'])
                    category_stats[category]['mrr'].append(qa_result['mrr'])
                    category_stats[category]['count'] += 1
                
            except Exception as e:
                print(f"Error evaluating sample {sample.get('sample_id', 'unknown')}: {e}")
                continue
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        overall_stats = {
            'total_questions': len(all_f1_scores),
            'average_f1': sum(all_f1_scores) / len(all_f1_scores) if all_f1_scores else 0,
            'average_em': sum(all_em_scores) / len(all_em_scores) if all_em_scores else 0,
            'average_evidence_precision': sum(all_evidence_precision) / len(all_evidence_precision) if all_evidence_precision else 0,
            'average_evidence_recall': sum(all_evidence_recall) / len(all_evidence_recall) if all_evidence_recall else 0,
            'average_evidence_f1': sum(all_evidence_f1) / len(all_evidence_f1) if all_evidence_f1 else 0,
            'average_recall_at_5': sum(all_recall_at_5) / len(all_recall_at_5) if all_recall_at_5 else 0,
            'average_recall_at_10': sum(all_recall_at_10) / len(all_recall_at_10) if all_recall_at_10 else 0,
            'average_mrr': sum(all_mrr) / len(all_mrr) if all_mrr else 0,
        }
        
        # è®¡ç®—åˆ†ç±»åˆ«ç»Ÿè®¡
        for category, stats in category_stats.items():
            stats['average_f1'] = sum(stats['f1_scores']) / len(stats['f1_scores']) if stats['f1_scores'] else 0
            stats['average_em'] = sum(stats['em_scores']) / len(stats['em_scores']) if stats['em_scores'] else 0
            stats['average_evidence_precision'] = sum(stats['evidence_precision']) / len(stats['evidence_precision']) if stats['evidence_precision'] else 0
            stats['average_evidence_recall'] = sum(stats['evidence_recall']) / len(stats['evidence_recall']) if stats['evidence_recall'] else 0
            stats['average_evidence_f1'] = sum(stats['evidence_f1']) / len(stats['evidence_f1']) if stats['evidence_f1'] else 0
            stats['average_recall_at_5'] = sum(stats['recall_at_5']) / len(stats['recall_at_5']) if stats['recall_at_5'] else 0
            stats['average_recall_at_10'] = sum(stats['recall_at_10']) / len(stats['recall_at_10']) if stats['recall_at_10'] else 0
            stats['average_mrr'] = sum(stats['mrr']) / len(stats['mrr']) if stats['mrr'] else 0
        
        # ä¿å­˜ç»“æœ
        output_data = {
            'evaluation_time': datetime.now().isoformat(),
            'data_file': self.data_file,
            'total_samples': len(eval_samples),
            'overall_stats': overall_stats,
            'category_stats': category_stats,
            'detailed_results': all_results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n=== TinyMem0 LoCoMo Evaluation Results ===")
        print(f"Total Questions: {overall_stats['total_questions']}")
        print(f"\n## QA Metrics ##")
        print(f"Overall F1 Score: {overall_stats['average_f1']:.4f}")
        print(f"Overall Exact Match: {overall_stats['average_em']:.4f}")
        print(f"\n## Evidence/Retrieval Metrics ##")
        print(f"Evidence Precision: {overall_stats['average_evidence_precision']:.4f}")
        print(f"Evidence Recall: {overall_stats['average_evidence_recall']:.4f}")
        print(f"Evidence F1: {overall_stats['average_evidence_f1']:.4f}")
        print(f"Recall@5: {overall_stats['average_recall_at_5']:.4f}")
        print(f"Recall@10: {overall_stats['average_recall_at_10']:.4f}")
        print(f"MRR: {overall_stats['average_mrr']:.4f}")
        print(f"\n## By Category ##")
        for category, stats in category_stats.items():
            print(f"Category {category}: F1={stats['average_f1']:.4f}, EM={stats['average_em']:.4f}, Ev-F1={stats['average_evidence_f1']:.4f}, MRR={stats['average_mrr']:.4f} ({stats['count']} questions)")
        
        print(f"\nDetailed results saved to: {output_file}")
        
        return output_data

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="åœ¨ LoCoMo æ•°æ®é›†ä¸Šè¯„æµ‹è®°å¿†ç³»ç»Ÿæ¨¡å‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å¯ç”¨çš„æ¨¡å‹ç³»ç»Ÿï¼š
  tinymem0    - TinyMem0 è®°å¿†ç³»ç»Ÿ (æ”¯æŒæœ¬åœ°LLMå’Œé˜¿é‡Œäº‘API)

ç¤ºä¾‹ç”¨æ³•ï¼š
  # è¯„æµ‹ TinyMem0 æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®é›†ï¼Œè¯„æµ‹5ä¸ªæ ·æœ¬
  python scripts/evaluate_system.py --model tinymem0 --num-samples 5
  
  # è¯„æµ‹æ‰€æœ‰æ ·æœ¬ï¼Œä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶
  python scripts/evaluate_system.py --model tinymem0 --output-file results.json
  
  # è¯„æµ‹æŒ‡å®šçš„æ ·æœ¬ID
  python scripts/evaluate_system.py --model tinymem0 --sample-ids sample_001 sample_002
        """
    )
    
    # å¿…é€‰å‚æ•°ï¼ˆé™¤éä½¿ç”¨ --list-modelsï¼‰
    parser.add_argument('--model', '-m', type=str,
                       choices=list(AVAILABLE_MODELS.keys()),
                       help='è¦è¯„æµ‹çš„æ¨¡å‹ç³»ç»Ÿåç§°')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--data-file', type=str, default='locomo/data/locomo10.json',
                       help='LoCoMo æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: locomo/data/locomo10.json)')
    parser.add_argument('--output-file', '-o', type=str, default=None,
                       help='è¯„æµ‹ç»“æœè¾“å‡ºæ–‡ä»¶ (é»˜è®¤: {model}_locomo_results.json)')
    parser.add_argument('--sample-ids', type=str, nargs='*',
                       help='æŒ‡å®šè¦è¯„æµ‹çš„æ ·æœ¬IDåˆ—è¡¨ (é»˜è®¤: è¯„æµ‹æ‰€æœ‰æ ·æœ¬)')
    parser.add_argument('--num-samples', '-n', type=int, default=None,
                       help='é™åˆ¶è¯„æµ‹çš„æ ·æœ¬æ•°é‡ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯• (é»˜è®¤: è¯„æµ‹æ‰€æœ‰æ ·æœ¬)')
    parser.add_argument('--list-models', action='store_true',
                       help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ç³»ç»Ÿ')
    
    args = parser.parse_args()
    
    # å¦‚æœè¯·æ±‚åˆ—å‡ºæ¨¡å‹ï¼Œæ˜¾ç¤ºåé€€å‡º
    if args.list_models:
        print("\nå¯ç”¨çš„è®°å¿†ç³»ç»Ÿæ¨¡å‹ï¼š")
        print("=" * 70)
        for model_id, config in AVAILABLE_MODELS.items():
            print(f"\n{model_id:15} - {config['name']}")
            print(f"{'':15}   {config['description']}")
            print(f"{'':15}   æ¨¡å—: {config['module']}.{config['class']}")
        print("\n" + "=" * 70)
        return
    
    # å¦‚æœä¸æ˜¯åˆ—å‡ºæ¨¡å‹ï¼Œåˆ™ --model å‚æ•°æ˜¯å¿…éœ€çš„
    if not args.model:
        parser.error("è¯„æµ‹æ¨¡å¼ä¸‹ --model/-m å‚æ•°æ˜¯å¿…éœ€çš„ã€‚ä½¿ç”¨ --list-models æŸ¥çœ‹å¯ç”¨æ¨¡å‹ã€‚")
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶å
    if args.output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output_file = f"{args.model}_locomo_results_{timestamp}.json"
    
    # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
    use_local_llm = os.getenv("USE_LOCAL_LLM", "false").lower() == "true"
    
    print("\n" + "=" * 70)
    print("è®°å¿†ç³»ç»Ÿè¯„æµ‹é…ç½®")
    print("=" * 70)
    print(f"æ¨¡å‹ç³»ç»Ÿ: {AVAILABLE_MODELS[args.model]['name']}")
    print(f"æ•°æ®æ–‡ä»¶: {args.data_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output_file}")
    
    if use_local_llm:
        # ä½¿ç”¨æœ¬åœ°LLMï¼Œæ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.getenv("LOCAL_MODEL_PATH"):
            print("\nâŒ é”™è¯¯: ä½¿ç”¨æœ¬åœ°LLMæ—¶å¿…é¡»è®¾ç½® LOCAL_MODEL_PATH ç¯å¢ƒå˜é‡")
            print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ï¼šLOCAL_MODEL_PATH=/path/to/your/model.gguf")
            return
        model_path = os.getenv("LOCAL_MODEL_PATH")
        if model_path and not os.path.exists(model_path):
            print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
            return
        print(f"LLMæ¨¡å¼: æœ¬åœ°æ¨¡å‹")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    else:
        # ä½¿ç”¨é˜¿é‡Œäº‘APIï¼Œæ£€æŸ¥APIå¯†é’¥
        if not os.getenv("DASHSCOPE_API_KEY"):
            print("\nâŒ é”™è¯¯: ä½¿ç”¨é˜¿é‡Œäº‘APIæ—¶å¿…é¡»è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
            print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ï¼šDASHSCOPE_API_KEY=your_api_key")
            return
        print(f"LLMæ¨¡å¼: é˜¿é‡Œäº‘ Dashscope API")

    # --- Ensure embedding model exists (if needed) ---------------------------------
    def _ensure_embedding_available(project_root: str) -> bool:
        """Ensure LOCAL_EMBEDDING_MODEL is set or download a default embedding model.

        Returns True if embedding model exists/was downloaded and env var is set, False otherwise.
        """
        local_emb = os.getenv("LOCAL_EMBEDDING_MODEL")
        if local_emb and os.path.exists(local_emb):
            print(f"Embedding: ä½¿ç”¨ LOCAL_EMBEDDING_MODEL={local_emb}")
            return True

        # try to find any model under ./embedding_models
        default_dir = os.path.join(project_root, 'embedding_models')
        if os.path.exists(default_dir) and any(os.scandir(default_dir)):
            # try to find a reasonable model directory (avoid .lock or temp files)
            def _find_model_dir(base_dir: str):
                model_files = ('pytorch_model.bin', 'model.safetensors', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack')
                for root, dirs, files in os.walk(base_dir):
                    # skip hidden/temp dirs
                    if os.path.basename(root).startswith('.'):
                        continue
                    for mf in model_files:
                        if mf in files:
                            return root
                # fallback: return first non-hidden child
                for entry in os.scandir(base_dir):
                    if not entry.name.startswith('.'):
                        return entry.path
                return None

            found = _find_model_dir(default_dir)
            if found:
                print(f"Embedding: åœ¨ {default_dir} å‘ç°æ¨¡å‹ï¼Œä½¿ç”¨: {found}")
                os.environ['LOCAL_EMBEDDING_MODEL'] = found
                return True

        # attempt to run the download_embedding.py script non-interactively
        download_script = os.path.join(project_root, 'scripts', 'download_embedding.py')
        if os.path.exists(download_script):
            print("\nâš™ï¸ æœªæ£€æµ‹åˆ°æœ¬åœ° embedding æ¨¡å‹ï¼Œæ­£åœ¨è‡ªåŠ¨ä¸‹è½½é»˜è®¤ embedding æ¨¡å‹åˆ° ./embedding_modelsï¼ˆå¯èƒ½éœ€è¦ç½‘ç»œï¼‰...")
            try:
                subprocess.run([sys.executable, download_script, '--model-id', '1', '--cache-dir', default_dir], check=True)
            except Exception as e:
                print(f"âŒ è‡ªåŠ¨ä¸‹è½½ embedding å¤±è´¥: {e}")
                return False

            # after download, find the actual model dir containing model files (avoid .lock)
            def _find_model_dir(base_dir: str):
                model_files = ('pytorch_model.bin', 'model.safetensors', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack')
                for root, dirs, files in os.walk(base_dir):
                    if os.path.basename(root).startswith('.'):
                        continue
                    for mf in model_files:
                        if mf in files:
                            return root
                # fallback: pick the largest non-hidden directory
                candidates = [entry for entry in os.scandir(base_dir) if entry.is_dir() and not entry.name.startswith('.')]
                if candidates:
                    candidates.sort(key=lambda e: e.stat().st_size if e.is_file() else 0, reverse=True)
                    return candidates[0].path
                return None

            found = _find_model_dir(default_dir)
            if found:
                os.environ['LOCAL_EMBEDDING_MODEL'] = found
                print(f"âœ… embedding å·²ä¸‹è½½å¹¶è®¾ç½® LOCAL_EMBEDDING_MODEL={found}")
                return True
        else:
            print("âš ï¸ æœªæ‰¾åˆ° scripts/download_embedding.pyï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ embeddingã€‚")

        print("âŒ embedding æ¨¡å‹ä¸å¯ç”¨ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œ scripts/download_embedding.py ä¸‹è½½æˆ–åœ¨ .env ä¸­é…ç½® LOCAL_EMBEDDING_MODEL")
        return False


    # --- Ensure local LLM model exists (if using local LLM) -----------------------
    def _ensure_local_llm(project_root: str) -> bool:
        """Ensure LOCAL_MODEL_PATH exists. If not, try to find a GGUF under ./models and set it.

        If a local model is still not found, suggest running scripts/download_model.py.
        """
        if os.getenv("USE_LOCAL_LLM", "false").lower() != "true":
            return True

        model_path = os.getenv("LOCAL_MODEL_PATH")
        if model_path and os.path.exists(model_path):
            print(f"LLM: ä½¿ç”¨ LOCAL_MODEL_PATH={model_path}")
            return True

        # search repo models/ for .gguf
        models_dir = os.path.join(project_root, 'models')
        if os.path.exists(models_dir):
            for fn in os.listdir(models_dir):
                if fn.lower().endswith('.gguf'):
                    candidate = os.path.join(models_dir, fn)
                    print(f"LLM: åœ¨ {models_dir} å‘ç° GGUF æ¨¡å‹: {candidate}ï¼Œå°†ä¸´æ—¶ä½¿ç”¨è¯¥è·¯å¾„ã€‚")
                    os.environ['LOCAL_MODEL_PATH'] = candidate
                    return True

        # no local model found â€” try helper script
        helper = os.path.join(project_root, 'scripts', 'download_model.py')
        if os.path.exists(helper):
            print("\nâš™ï¸ æœªæ£€æµ‹åˆ°æœ¬åœ° LLM æ¨¡å‹ï¼Œå°è¯•è¿è¡Œ scripts/download_model.py æ¥å¸®åŠ©é…ç½®ï¼ˆä¸ä¼šè‡ªåŠ¨ä¸‹è½½å¤§æ¨¡å‹ï¼‰ã€‚")
            try:
                subprocess.run([sys.executable, helper], check=False)
            except Exception:
                pass

        model_path = os.getenv("LOCAL_MODEL_PATH")
        if model_path and os.path.exists(model_path):
            print(f"LLM: å·²é…ç½® LOCAL_MODEL_PATH={model_path}")
            return True

        print("âŒ æœ¬åœ° LLM æ¨¡å‹ä»ä¸å¯ç”¨ã€‚è¯·å°† GGUF æ¨¡å‹æ”¾å…¥é¡¹ç›®çš„ models/ ç›®å½•ï¼Œæˆ–åœ¨ .env ä¸­è®¾ç½® LOCAL_MODEL_PATHã€‚")
        return False

    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.data_file):
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {args.data_file}")
        print("è¯·ç¡®ä¿ LoCoMo æ•°æ®é›†å·²ä¸‹è½½å¹¶æ”¾åœ¨æ­£ç¡®çš„ä½ç½®")
        return
    
    print("=" * 70 + "\n")
    
    # ç¡®ä¿ embedding å¯ç”¨
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if not _ensure_embedding_available(project_root):
        return

    # ç¡®ä¿æœ¬åœ°LLMï¼ˆå¦‚ä½¿ç”¨ï¼‰å¯ç”¨æˆ–å·²æç¤ºç”¨æˆ·
    if not _ensure_local_llm(project_root):
        return

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = LoCoMoEvaluator(args.data_file)
    
    # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    sample_ids = args.sample_ids
    if args.num_samples and not sample_ids:
        sample_ids = [s['sample_id'] for s in evaluator.samples[:args.num_samples]]
        print(f"ğŸ“Š å°†è¯„æµ‹å‰ {args.num_samples} ä¸ªæ ·æœ¬\n")
    elif sample_ids:
        print(f"ğŸ“Š å°†è¯„æµ‹æŒ‡å®šçš„ {len(sample_ids)} ä¸ªæ ·æœ¬\n")
    else:
        print(f"ğŸ“Š å°†è¯„æµ‹æ‰€æœ‰ {len(evaluator.samples)} ä¸ªæ ·æœ¬\n")
    
    # è¿è¡Œè¯„ä¼°
    try:
        results = evaluator.evaluate_memory_system(args.model, args.output_file, sample_ids)
        
        if results:
            print("\nâœ… è¯„æµ‹å®Œæˆï¼")
            return results
        else:
            print("\nâŒ è¯„æµ‹å¤±è´¥")
            return None
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è¯„æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
        return None
    except Exception as e:
        print(f"\nâŒ è¯„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()